{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##### *数据预处理*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### *将文本拆分为 tokens，采用Llama*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import jieba\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# linly-ai/chinese-llama-2-7b\n",
    "# 'NousResearch/Llama-2-7b-hf'\n",
    "# 'hfl/llama-3-chinese-8b-instruct-v3'\n",
    "tokenizer = AutoTokenizer.from_pretrained('NousResearch/Llama-2-7b-hf')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 29871, 30505, 30651, 30822, 30210, 30287, 31559, 30594, 30867, 30275, 30214, 235, 178, 190, 30767, 233, 159, 142, 31373, 31381, 30998, 235, 184, 179, 31138, 30672, 30505, 31138, 31475, 30210, 30287, 30470, 30275, 235, 184, 179, 31138, 30210, 234, 181, 193, 30648, 232, 145, 137, 31101, 30214, 232, 160, 169, 234, 145, 138, 30533, 31639, 30214, 30672, 30413, 31043, 30397, 30919, 31381, 30998, 30505, 30810, 31217, 236, 190, 148, 233, 157, 154, 235, 178, 164, 232, 191, 133, 30210, 235, 194, 186, 236, 131, 151, 30429, 31811, 30780, 231, 190, 131, 31882, 30214, 30672, 232, 193, 139, 30413, 30670, 30267], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\ninput_ids 是句子中每个 token 对应的索引。\\nattention_mask 指示是否应关注该 token。\\ntoken_type_ids 在有多个序列时标识一个 token 属于哪个序列。\\n'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '在以后的一段时光中，读者朋友们将走过我在过去的一年中走过的精神历程，坦率地说，我不知道你们将在这条黑暗诡异的迷途上看到什么，我很不安。'\n",
    "\n",
    "encoded_input = tokenizer(sentence)\n",
    "print(encoded_input)\n",
    "\n",
    "'''\n",
    "input_ids 是句子中每个 token 对应的索引。\n",
    "attention_mask 指示是否应关注该 token。\n",
    "token_type_ids 在有多个序列时标识一个 token 属于哪个序列。\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'<s> 在以后的一段时光中，读者朋友们将走过我在过去的一年中走过的精神历程，坦率地说，我不知道你们将在这条黑暗诡异的迷途上看到什么，我很不安。'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 显示解码信息\n",
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### *配置模型参数*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 模型配置\n",
    "from transformers import AutoConfig\n",
    "\n",
    "hidden_size = 256\n",
    "# 中间层取 8/3 倍，按 128 向上取整\n",
    "intermediate_size = (int(hidden_size * 8/3 / 128) + 1) * 128\n",
    "\n",
    "config = AutoConfig.for_model(\n",
    "    # 模型的类型，这里是\"llama\"\n",
    "    model_type=\"llama\",\n",
    "    # 隐藏层的大小\n",
    "    hidden_size=hidden_size,\n",
    "    # 中间层的大小\n",
    "    intermediate_size=intermediate_size,\n",
    "    # 在自注意力机制中，每个注意力头的数量\n",
    "    num_attention_heads=16,\n",
    "    # 模型中隐藏层的数量（即Transformer编码器的堆叠次数）\n",
    "    num_hidden_layers=4,\n",
    "    num_key_value_heads=8                  # 分为 8 组\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "LlamaConfig {\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 256,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 768,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 4,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.41.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### *模型实例化*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_config(\n",
    "    config,\n",
    "    torch_dtype = torch.float32   # 全精度训练\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 256)\n    (layers): ModuleList(\n      (0-3): 4 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=256, out_features=256, bias=False)\n          (k_proj): Linear(in_features=256, out_features=128, bias=False)\n          (v_proj): Linear(in_features=256, out_features=128, bias=False)\n          (o_proj): Linear(in_features=256, out_features=256, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=256, out_features=768, bias=False)\n          (up_proj): Linear(in_features=256, out_features=768, bias=False)\n          (down_proj): Linear(in_features=768, out_features=256, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=256, out_features=32000, bias=False)\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### *打印参数*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name & Parameters\n",
      "----------------------------\n",
      "model.embed_tokens.weight                          | Size: torch.Size([32000, 256])       | Count: 8192000             \n",
      "model.layers.0.self_attn.q_proj.weight             | Size: torch.Size([256, 256])         | Count: 65536               \n",
      "model.layers.0.self_attn.k_proj.weight             | Size: torch.Size([128, 256])         | Count: 32768               \n",
      "model.layers.0.self_attn.v_proj.weight             | Size: torch.Size([128, 256])         | Count: 32768               \n",
      "model.layers.0.self_attn.o_proj.weight             | Size: torch.Size([256, 256])         | Count: 65536               \n",
      "model.layers.0.mlp.gate_proj.weight                | Size: torch.Size([768, 256])         | Count: 196608              \n",
      "model.layers.0.mlp.up_proj.weight                  | Size: torch.Size([768, 256])         | Count: 196608              \n",
      "model.layers.0.mlp.down_proj.weight                | Size: torch.Size([256, 768])         | Count: 196608              \n",
      "model.layers.0.input_layernorm.weight              | Size: torch.Size([256])              | Count: 256                 \n",
      "model.layers.0.post_attention_layernorm.weight     | Size: torch.Size([256])              | Count: 256                 \n",
      "model.layers.1.self_attn.q_proj.weight             | Size: torch.Size([256, 256])         | Count: 65536               \n",
      "model.layers.1.self_attn.k_proj.weight             | Size: torch.Size([128, 256])         | Count: 32768               \n",
      "model.layers.1.self_attn.v_proj.weight             | Size: torch.Size([128, 256])         | Count: 32768               \n",
      "model.layers.1.self_attn.o_proj.weight             | Size: torch.Size([256, 256])         | Count: 65536               \n",
      "model.layers.1.mlp.gate_proj.weight                | Size: torch.Size([768, 256])         | Count: 196608              \n",
      "model.layers.1.mlp.up_proj.weight                  | Size: torch.Size([768, 256])         | Count: 196608              \n",
      "model.layers.1.mlp.down_proj.weight                | Size: torch.Size([256, 768])         | Count: 196608              \n",
      "model.layers.1.input_layernorm.weight              | Size: torch.Size([256])              | Count: 256                 \n",
      "model.layers.1.post_attention_layernorm.weight     | Size: torch.Size([256])              | Count: 256                 \n",
      "model.layers.2.self_attn.q_proj.weight             | Size: torch.Size([256, 256])         | Count: 65536               \n",
      "model.layers.2.self_attn.k_proj.weight             | Size: torch.Size([128, 256])         | Count: 32768               \n",
      "model.layers.2.self_attn.v_proj.weight             | Size: torch.Size([128, 256])         | Count: 32768               \n",
      "model.layers.2.self_attn.o_proj.weight             | Size: torch.Size([256, 256])         | Count: 65536               \n",
      "model.layers.2.mlp.gate_proj.weight                | Size: torch.Size([768, 256])         | Count: 196608              \n",
      "model.layers.2.mlp.up_proj.weight                  | Size: torch.Size([768, 256])         | Count: 196608              \n",
      "model.layers.2.mlp.down_proj.weight                | Size: torch.Size([256, 768])         | Count: 196608              \n",
      "model.layers.2.input_layernorm.weight              | Size: torch.Size([256])              | Count: 256                 \n",
      "model.layers.2.post_attention_layernorm.weight     | Size: torch.Size([256])              | Count: 256                 \n",
      "model.layers.3.self_attn.q_proj.weight             | Size: torch.Size([256, 256])         | Count: 65536               \n",
      "model.layers.3.self_attn.k_proj.weight             | Size: torch.Size([128, 256])         | Count: 32768               \n",
      "model.layers.3.self_attn.v_proj.weight             | Size: torch.Size([128, 256])         | Count: 32768               \n",
      "model.layers.3.self_attn.o_proj.weight             | Size: torch.Size([256, 256])         | Count: 65536               \n",
      "model.layers.3.mlp.gate_proj.weight                | Size: torch.Size([768, 256])         | Count: 196608              \n",
      "model.layers.3.mlp.up_proj.weight                  | Size: torch.Size([768, 256])         | Count: 196608              \n",
      "model.layers.3.mlp.down_proj.weight                | Size: torch.Size([256, 768])         | Count: 196608              \n",
      "model.layers.3.input_layernorm.weight              | Size: torch.Size([256])              | Count: 256                 \n",
      "model.layers.3.post_attention_layernorm.weight     | Size: torch.Size([256])              | Count: 256                 \n",
      "model.norm.weight                                  | Size: torch.Size([256])              | Count: 256                 \n",
      "lm_head.weight                                     | Size: torch.Size([32000, 256])       | Count: 8192000             \n",
      "----------------------------\n",
      "Total Parameters: 19532032 (19.5 M)\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer Name & Parameters\")\n",
    "print(\"----------------------------\")\n",
    "total_params = 0\n",
    "for name, parameter in model.named_parameters():\n",
    "    param_size = parameter.size()\n",
    "    param_count = torch.prod(torch.tensor(param_size)).item()\n",
    "    total_params += param_count\n",
    "    print(f\"{name:50} | Size: {str(param_size):30} | Count: {str(param_count):20}\")\n",
    "print(\"----------------------------\")\n",
    "print(f\"Total Parameters: {total_params} ({total_params / 1000000:.1f} M)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### *未训练模型推理*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三体吻堭泪，他们恩我\n"
     ]
    }
   ],
   "source": [
    "def inference(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    input_text: str = \"三体\",\n",
    "    max_new_tokens: int = 16\n",
    "):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    # print(inputs)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_k=40,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    # print(outputs)\n",
    "    generated_text = tokenizer.decode(\n",
    "        outputs[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    # print(outputs)\n",
    "    print(generated_text)\n",
    "\n",
    "inference(model , tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### *模型参数初始化*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name and param.dim() > 1:\n",
    "        torch.nn.init.kaiming_uniform_(param, mode='fan_in', nonlinearity='leaky_relu')\n",
    "    elif 'bias' in name:\n",
    "        # 一般偏置项可以初始化为0\n",
    "        torch.nn.init.constant_(param, 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### *读入数据*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 210970\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 52743\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "path = 'D:\\\\pro_of_program\\\\practical-training-projects\\\\checkdata'\n",
    "ds_train = load_dataset(path , split='train[:80%]')    # 取80进行训练\n",
    "ds_val = load_dataset(path , split='train[80%:]')\n",
    "\n",
    "print(ds_train)\n",
    "print(ds_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['三体》终于能与科幻朋友们见面了，用连载的方式事先谁都没有想到，也是无奈之举。之前就题材问题与编辑们仔细商讨过，感觉没有什么问题，但没想到今年是文革三十周年这事儿，单行本一时出不了，也只能这样了。', '其实这本书不是文革题材的，文革内容在其中只占不到十分之一，但却是一个漂荡在故事中挥之不去的精神幽灵。', '本书虽不是《球状闪电》的续集，但可以看做那个故事所发生的世界在其后的延续，那个物理学家在故事中出现但已不重要，其他的人则永远消失了，林云真的死了，虽然我有时在想，如果她活下来，最后是不是这个主人公的样子？', '这是一个暂名为《地球往事》的系列的第一部，可以看做一个更长的故事的开始。']}\n"
     ]
    }
   ],
   "source": [
    "print(ds_train[:4])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### *token化*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def process(sentences):\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('hfl/llama-3-chinese-8b-instruct-v3')\n",
    "\n",
    "    max_token = 4096 # 最长token数\n",
    "    '''\n",
    "    sentences['text'] = List[str]\n",
    "    add_special_tokens = False 表示让tokenizer不要加上特殊 token\n",
    "    llama不需要举手标记\n",
    "    '''\n",
    "    text = tokenizer(sentences['text'] , add_special_tokens = False)\n",
    "    input_text = text['input_ids']\n",
    "    check_input , check_mask = [] , []\n",
    "    for iids in input_text:\n",
    "        # 直接截断 + 尾部结束标志\n",
    "        temp = iids[-max_token + 1 : ] + [tokenizer.eos_token_id]\n",
    "        check_input.append(temp)\n",
    "        check_mask.append([1] * len(temp)) # attention_mask保持于数据的一致\n",
    "\n",
    "    return {\n",
    "        \"input_ids\" : check_input,\n",
    "        \"attention_mask\" : check_mask\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "Running tokenizer on train_set:  (num_proc=8):   0%|          | 0/210970 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c10a64df981445287ccf3b54bd9d7e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Running tokenizer on val_set:  (num_proc=8):   0%|          | 0/52743 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e8e95a29c5f4cbcb099cc9ec5cc6cdd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 210970\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 52743\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds_train = ds_train.shuffle()\n",
    "ds_train = ds_train.map(\n",
    "    process,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=ds_train.column_names,\n",
    "    desc='Running tokenizer on train_set: '\n",
    ")\n",
    "\n",
    "ds_val = ds_val.map(\n",
    "    process,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=ds_val.column_names,\n",
    "    desc='Running tokenizer on val_set: '\n",
    ")\n",
    "\n",
    "print(ds_train)\n",
    "print(ds_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [262, 44689, 124669, 30926, 14382, 103478, 25333, 106625, 61786, 13, 10447, 241, 99, 61786, 70349, 113280, 104083, 103698, 125653, 104105, 100823, 116693, 103001, 109, 21043, 17885, 103, 67178, 9554, 103054, 11571, 89151, 116693, 103001, 109, 93233, 106837, 3922, 86348, 116693, 52332, 72368, 115056, 107711, 58850, 35147, 128009], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(ds_train[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### *参数配置*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='D:\\\\pro_of_program\\\\practical-training-projects\\\\tf', # 输出路径\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_steps=1000,\n",
    "    per_device_train_batch_size=4, # 训练集batch_size\n",
    "    gradient_accumulation_steps=1, # 梯度累计步大小，省显存，但小模型没必要，用 1 收敛比较快\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type='cosine', # 学习率调度策略，LLM 训练一般都用余弦\n",
    "    logging_steps=50, # 打印间隔\n",
    "    report_to=\"tensorboard\",\n",
    "    bf16=torch.cuda.is_bf16_supported(),        # 尝试配置 bf16\n",
    "    fp16=not torch.cuda.is_bf16_supported(),    # bf16 不行就上 fp16\n",
    "    num_train_epochs=1,\n",
    "    save_steps=1000, # 检查点保存步骤间隔\n",
    "    save_total_limit=2, # output_dir 内留存的检查点最大数目\n",
    "    seed=5024\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[43], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# 把输入偏移一位当作预测目标\u001B[39;00m\n\u001B[0;32m      9\u001B[0m data_collator \u001B[38;5;241m=\u001B[39m DataCollatorForLanguageModeling(tokenizer\u001B[38;5;241m=\u001B[39mtokenizer, mlm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m---> 11\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m                    \u001B[49m\u001B[38;5;66;43;03m# 模型实例\u001B[39;49;00m\n\u001B[0;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m             \u001B[49m\u001B[38;5;66;43;03m# 训练参数\u001B[39;49;00m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mds_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m         \u001B[49m\u001B[38;5;66;43;03m# 训练集\u001B[39;49;00m\n\u001B[0;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mds_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# 验证集（评估集）\u001B[39;49;00m\n\u001B[0;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# 分词器\u001B[39;49;00m\n\u001B[0;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_collator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_collator\u001B[49m\u001B[43m   \u001B[49m\u001B[38;5;66;43;03m# data collator\u001B[39;49;00m\n\u001B[0;32m     18\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Python 3.11\\Lib\\site-packages\\transformers\\trainer.py:397\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001B[0m\n\u001B[0;32m    395\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs \u001B[38;5;241m=\u001B[39m args\n\u001B[0;32m    396\u001B[0m \u001B[38;5;66;03m# Seed must be set before instantiating the model when using model\u001B[39;00m\n\u001B[1;32m--> 397\u001B[0m enable_full_determinism(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mseed) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mfull_determinism \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mset_seed\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    398\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhp_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    399\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeepspeed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Python 3.11\\Lib\\site-packages\\transformers\\trainer_utils.py:99\u001B[0m, in \u001B[0;36mset_seed\u001B[1;34m(seed, deterministic)\u001B[0m\n\u001B[0;32m     97\u001B[0m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mseed(seed)\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_torch_available():\n\u001B[1;32m---> 99\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmanual_seed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    100\u001B[0m     torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mmanual_seed_all(seed)\n\u001B[0;32m    101\u001B[0m     \u001B[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Python 3.11\\Lib\\site-packages\\torch\\random.py:40\u001B[0m, in \u001B[0;36mmanual_seed\u001B[1;34m(seed)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcuda\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39m_is_in_bad_fork():\n\u001B[1;32m---> 40\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmanual_seed_all\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmps\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mmps\u001B[38;5;241m.\u001B[39m_is_in_bad_fork():\n",
      "File \u001B[1;32mD:\\Python 3.11\\Lib\\site-packages\\torch\\cuda\\random.py:113\u001B[0m, in \u001B[0;36mmanual_seed_all\u001B[1;34m(seed)\u001B[0m\n\u001B[0;32m    110\u001B[0m         default_generator \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdefault_generators[i]\n\u001B[0;32m    111\u001B[0m         default_generator\u001B[38;5;241m.\u001B[39mmanual_seed(seed)\n\u001B[1;32m--> 113\u001B[0m \u001B[43m_lazy_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed_all\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Python 3.11\\Lib\\site-packages\\torch\\cuda\\__init__.py:183\u001B[0m, in \u001B[0;36m_lazy_call\u001B[1;34m(callable, **kwargs)\u001B[0m\n\u001B[0;32m    181\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_lazy_call\u001B[39m(\u001B[38;5;28mcallable\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    182\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_initialized():\n\u001B[1;32m--> 183\u001B[0m         \u001B[38;5;28;43mcallable\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    185\u001B[0m         \u001B[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001B[39;00m\n\u001B[0;32m    186\u001B[0m         \u001B[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001B[39;00m\n\u001B[0;32m    187\u001B[0m         \u001B[38;5;66;03m# else here if this ends up being important.\u001B[39;00m\n\u001B[0;32m    188\u001B[0m         \u001B[38;5;28;01mglobal\u001B[39;00m _lazy_seed_tracker\n",
      "File \u001B[1;32mD:\\Python 3.11\\Lib\\site-packages\\torch\\cuda\\random.py:111\u001B[0m, in \u001B[0;36mmanual_seed_all.<locals>.cb\u001B[1;34m()\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(device_count()):\n\u001B[0;32m    110\u001B[0m     default_generator \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdefault_generators[i]\n\u001B[1;32m--> 111\u001B[0m     \u001B[43mdefault_generator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmanual_seed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "torch.cuda.device_count()\n",
    "# 把输入偏移一位当作预测目标\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                    # 模型实例\n",
    "    args=args,             # 训练参数\n",
    "    train_dataset=ds_train,         # 训练集\n",
    "    eval_dataset=ds_val,            # 验证集（评估集）\n",
    "    tokenizer=tokenizer,            # 分词器\n",
    "    data_collator=data_collator   # data collator\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train().to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_path = 'D:\\\\pro_of_program\\\\practical-training-projects\\\\model\\\\TinyLlamaModel'\n",
    "model.save_pretrained(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mD:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mpro_of_program\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mpractical-training-projects\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mLlama3_8b_novel\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# tokenizer = tokenizer.from_pretrained(path)\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mfrom_pretrained(path)\n\u001B[0;32m      4\u001B[0m inference(model , tokenizer , \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m孙悟空来啦\u001B[39m\u001B[38;5;124m\"\u001B[39m , max_new_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m256\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "path = 'D:\\\\pro_of_program\\\\practical-training-projects\\\\model\\\\TinyLlamaModel'\n",
    "# tokenizer = tokenizer.from_pretrained(path)\n",
    "model = model.from_pretrained(path)\n",
    "inference(model , tokenizer , \"孙悟空来啦\" , max_new_tokens = 256)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model D:\\pro_of_program\\practical-training-projects\\model\\chat_Tiny_Llama with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"D:\\Python 3.11\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Python 3.11\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 566, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GemmaConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, JambaConfig, JetMoeConfig, LlamaConfig, MambaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, OlmoConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig.\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"D:\\Python 3.11\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Python 3.11\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 566, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pipeline\n\u001B[0;32m      2\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m我比现在年轻十岁的时候，获得了一个游手好闲的职业\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 3\u001B[0m generator \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext-generation\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mD:\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43mpro_of_program\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43mpractical-training-projects\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43mchat_Tiny_Llama\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m res \u001B[38;5;241m=\u001B[39m generator(text)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(res)\n",
      "File \u001B[1;32mD:\\Python 3.11\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:906\u001B[0m, in \u001B[0;36mpipeline\u001B[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n\u001B[0;32m    904\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m framework \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    905\u001B[0m     model_classes \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m: targeted_task[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m: targeted_task[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m]}\n\u001B[1;32m--> 906\u001B[0m     framework, model \u001B[38;5;241m=\u001B[39m \u001B[43minfer_framework_load_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    907\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    908\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_classes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    909\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    910\u001B[0m \u001B[43m        \u001B[49m\u001B[43mframework\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mframework\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    911\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    912\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    913\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    914\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    916\u001B[0m model_config \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mconfig\n\u001B[0;32m    917\u001B[0m hub_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39m_commit_hash\n",
      "File \u001B[1;32mD:\\Python 3.11\\Lib\\site-packages\\transformers\\pipelines\\base.py:296\u001B[0m, in \u001B[0;36minfer_framework_load_model\u001B[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001B[0m\n\u001B[0;32m    294\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m class_name, trace \u001B[38;5;129;01min\u001B[39;00m all_traceback\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    295\u001B[0m             error \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhile loading with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclass_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, an error is thrown:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mtrace\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 296\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    297\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not load model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with any of the following classes: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclass_tuple\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. See the original errors:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00merror\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    298\u001B[0m         )\n\u001B[0;32m    300\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m framework \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    301\u001B[0m     framework \u001B[38;5;241m=\u001B[39m infer_framework(model\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Could not load model D:\\pro_of_program\\practical-training-projects\\model\\chat_Tiny_Llama with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"D:\\Python 3.11\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Python 3.11\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 566, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GemmaConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, JambaConfig, JetMoeConfig, LlamaConfig, MambaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, OlmoConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig.\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"D:\\Python 3.11\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Python 3.11\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 566, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "text = '我比现在年轻十岁的时候，获得了一个游手好闲的职业'\n",
    "generator = pipeline('text-generation' , model = 'D:\\\\pro_of_program\\\\practical-training-projects\\\\model\\\\chat_Tiny_Llama')\n",
    "res = generator(text)\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《西游记》是中国古代神话中的一部经典作品，讲述了孙悟空和他的伙伴们历经九九八十一难，历经千辛万苦，最终取得真经的故事。在这个过程中，他们经历了无数的磨难和考验，但他们从未放弃。孙悟空是一个勇敢的战士，他用自己的智慧和勇气，克服了无数的难关，最终取得了真经。\n",
      "而在《三体飞船》中，孙悟空则是一个聪明、机智的外星人，他善于运用各种科技手段，与三体文明进行了一场惊心动魄的战斗。在这场激烈的战斗中，孙悟空展现了自己的勇气和智慧，战胜了三体文明的强大力量。\n",
      "这就是《西游记》中的情节，它是一部充满智慧、勇气和智慧的经典作品。它不仅展示了孙悟空的勇气，更展示了他与三体文明之间的深厚友谊。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model_id = 'D:\\\\pro_of_program\\\\practical-training-projects\\\\model\\\\chat_Tiny_Llama'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, trust_remote_code=True).to(device)\n",
    "\n",
    "txt = '续写：孙悟空攻击三体飞船'\n",
    "\n",
    "encode_ids = tokenizer([txt])\n",
    "input_ids, attention_mask = torch.LongTensor(encode_ids['input_ids']), torch.LongTensor(encode_ids['attention_mask'])\n",
    "\n",
    "outs = model.my_generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    attention_mask=attention_mask.to(device),\n",
    "    max_seq_len=256,\n",
    "    search_type='beam',\n",
    ")\n",
    "\n",
    "outs_txt = tokenizer.batch_decode(outs.cpu().numpy(), skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "print(outs_txt[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
